{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max Scaling is a Normalization technique, its objective is to transform the value between [0,1].\n",
    "It is calculated as: \n",
    ">                                   x_scaled = x_i - x_min / x_max - x_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[76.86516853932584,\n",
       " 61.865168539325836,\n",
       " 52.865168539325836,\n",
       " -0.13483146067415674]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Using the Formula\n",
    "income = [89,74,65,12]\n",
    "scaled_income = []\n",
    "for i in income:\n",
    "    output = (i - min(income) / max(income) - min(income))\n",
    "    scaled_income.append(output)\n",
    "\n",
    "scaled_income\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17073171],\n",
       "       [0.6097561 ],\n",
       "       [1.        ],\n",
       "       [0.        ],\n",
       "       [0.75609756]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the MinMaxScaler Library\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "min_max = MinMaxScaler()\n",
    "\n",
    "data = pd.DataFrame([41,59,75,34,65])\n",
    "min_max.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit Vector: A unit vector refers to a vector that has a magnitude of 1. Feature scaling is a technique used in machine learning and data preprocessing to bring all the features (variables) of a dataset to a similar scale or range. \n",
    "\n",
    "Differences:\n",
    "* Unit vector Scaling ensures that each vector has a magnitude of 1 while maintaining its direction. Min-Max scaling rescales features to a specific range, usually between 0 and 1.\n",
    "* Unit vector is focused on the direction of the vectors while min max scaler is focused on bring the data points in certain range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78575061, 0.26449371, 0.46438248, 0.31142891]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "lst = [[1423,479,841,564]]\n",
    "normalize(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA stands for Principle Component Analysis is a dimensionality reduction technique used in machine learning. Its aims is to reduces the number of feature from a dataset while preserving the most important patterns and varaiaiton from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) and feature extraction are closely related concepts, and PCA can be used as a feature extraction technique. Feature extraction is the process of transforming the original features of a dataset into a new set of features that represent the data more effectively or efficiently. PCA is a specific feature extraction method that aims to reduce the dimensionality of the data while retaining the most important information.\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "Data Preparation: Start with a dataset that contains multiple features (high-dimensional data).\n",
    "\n",
    "Standardize the Data: To ensure that all features contribute equally to PCA, it is essential to standardize the data (mean=0, standard deviation=1).\n",
    "\n",
    "Apply PCA: Apply PCA to the standardized data to transform the features into a new set of uncorrelated variables called principal components.\n",
    "\n",
    "Choose the Number of Principal Components: Select the number of principal components to retain based on the desired level of dimensionality reduction or the explained variance (e.g., retaining 95% or 99% of the total variance).\n",
    "\n",
    "Project Data onto New Feature Space: Multiply the original data by the selected principal components to obtain the new, lower-dimensional representation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Understand the Data:\n",
    "\n",
    "First, you need to explore and understand the dataset containing features like price, rating, and delivery time. Check for any missing values, outliers, or data inconsistencies that need to be handled before scaling the data.\n",
    "\n",
    "Step 2: Apply Min-Max Scaling:\n",
    "\n",
    "Once you've checked the data for any issues, you can proceed with Min-Max scaling. For each feature, apply the following formula to scale the data to the desired range (typically between 0 and 1):\n",
    "\n",
    "For a feature X, Min-Max scaling is calculated as:\n",
    "X_scaled = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "Here, X_min is the minimum value of the feature, and X_max is the maximum value of the feature.\n",
    "\n",
    "\n",
    "Step 3: Implement Min-Max Scaling in Your Code:\n",
    "\n",
    "Depending on the programming language or library you are using for your recommendation system, there are often built-in functions or libraries that can perform Min-Max scaling for you. For example, in Python, you can use the scikit-learn library's MinMaxScaler class to perform Min-Max scaling on the dataset.\n",
    "\n",
    "Step 4: Use the Scaled Data for Recommendation:\n",
    "\n",
    "Once the data is scaled using Min-Max scaling, the features like price, rating, and delivery time will all have values between 0 and 1. This ensures that no single feature dominates the recommendation process based on its scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Data Preparation:\n",
    "\n",
    "Gather the dataset that includes various features related to company financial data (e.g., revenue, earnings, debt, etc.) and market trends (e.g., interest rates, GDP, inflation, etc.).\n",
    "Ensure that the data is cleaned and preprocessed. Handle any missing values, outliers, or data inconsistencies that might affect the PCA.\n",
    "\n",
    "**Step 2:** Standardize the Data:\n",
    "\n",
    "Standardize the features in the dataset to have a mean of 0 and a standard deviation of 1. This step is essential for PCA, as it ensures that all features contribute equally to the principal components and prevents any feature from dominating the analysis due to its larger scale.\n",
    "\n",
    "**Step 3:** Apply PCA:\n",
    "\n",
    "Use PCA to transform the standardized data into a new set of uncorrelated variables called principal components. These principal components represent the directions of maximum variance in the data.\n",
    "\n",
    "**Step 4:** Determine the Number of Principal Components:\n",
    "\n",
    "Decide on the number of principal components to retain based on the desired level of dimensionality reduction or the explained variance. Retaining a sufficient number of principal components is essential to capture most of the variability in the data while reducing the dimensionality.\n",
    "\n",
    "**Step 5:** Project Data onto New Feature Space:\n",
    "\n",
    "Project the original data onto the new feature space represented by the selected principal components. This is achieved by taking the dot product of the data matrix and the matrix containing the selected eigenvectors.\n",
    "\n",
    "**Step 6:** Model Building and Prediction:\n",
    "\n",
    "Use the reduced set of principal components as the new features for building your stock price prediction model. You can use various regression or time series forecasting algorithms to create the model.\n",
    "\n",
    "Once the model is trained, you can make predictions on new data using the same set of principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.21052632],\n",
       "       [0.47368421],\n",
       "       [0.73684211],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([1,5,10,15,20])\n",
    "min_max.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform feature extraction using PCA for the given dataset with features [height, weight, age, gender, blood pressure], we need to follow these steps:\n",
    "\n",
    "Step 1: Standardize the Data:\n",
    "- Start by standardizing the data to ensure that all features are on the same scale (mean=0, standard deviation=1). This step is crucial for PCA to work effectively.\n",
    "\n",
    "Step 2: Apply PCA:\n",
    "- Compute the covariance matrix of the standardized data and find the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "Step 3: Sort Eigenvectors by Eigenvalues:\n",
    "- Sort the eigenvectors in descending order based on their corresponding eigenvalues. The eigenvectors with higher eigenvalues explain more variance in the data and are, therefore, more important.\n",
    "\n",
    "Step 4: Choose the Number of Principal Components:\n",
    "- Decide on the number of principal components to retain based on the desired level of dimensionality reduction or the explained variance. One common approach is to set a threshold for the cumulative explained variance (e.g., 95% or 99%). Retaining enough principal components to reach this threshold will capture a significant amount of information in the data.\n",
    "\n",
    "Step 5: Project Data onto New Feature Space:\n",
    "- Project the original data onto the new feature space represented by the selected principal components. This is done by taking the dot product of the data matrix and the matrix containing the selected eigenvectors.\n",
    "\n",
    "The number of principal components to retain depends on the specific dataset and the desired level of dimensionality reduction. By looking at the cumulative explained variance, we can determine how much of the total variance in the data is captured by each additional principal component."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
